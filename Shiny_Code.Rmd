---
title: "Shiny_Code"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(GEOquery)  ## go to https://github.com/seandavi/GEOquery for installation details
library(ggplot2)
library(tidyverse)
library(reshape2)

```

```{r}

clinical_outcome_GSE107509 <- getGEO("GSE107509")

clinical_outcome_GSE107509_results <- clinical_outcome_GSE107509$GSE107509_series_matrix.txt.gz$characteristics_ch1

GSE107509_rejection_status <- unlist(lapply( strsplit(as.character(clinical_outcome_GSE107509_results), ": " ) , `[[` , 2)  )

GSE107509_rejection_status = toupper(GSE107509_rejection_status)

rejection_table = table(GSE107509_rejection_status)  # WILL NEED TO COMBINE THESE: "subclinical acute rejection" "subclinical Acute Rejection" since they are the same

print(names(rejection_table)[[1]])
print(as.vector(rejection_table[1]))


# need to add on the outcome as cbind() after using transpose t() 

```

```{r}

#write.csv(data.frame(rejection_status_GSE107509), "data/rejection_status_GSE107509.txt", row.names=FALSE)

GSE107509_rejection_status = read.csv("data/rejection_status_GSE107509.txt")

```

```{r}

gse = readr::read_csv("data/GSE107509_RAW/GSE107509_Matrix.txt")

# ------ read_csv returns a tibble but can be converted to a t=data frame as.data.frame(gse)

# Get gene names
gene_names = gse$Gene

# Remove first column - The X column we saved
gse = gse[,-1]

# Set row names as gene names
rownames(gse) = gene_names

# Transpose the data frame
gse_t = as.data.frame(t(gse))
colnames(gse_t) = gene_names


```

```{r}

# check transformation
summary(gse[,1])

p <- ggplot(melt(gse), aes(x=variable, y=value)) +  
  geom_boxplot(outlier.colour="black", outlier.shape=16,
             outlier.size=0.5, notch=FALSE) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs (x = "patient", y = "expression value") + theme_minimal()
p
```


```{r}

cor_matrix<-cor(gse_t[1:100])
diag(cor_matrix)<-0
library(corrplot)
corrplot(cor_matrix, method="square")

```


```{r}

prComp<-prcomp(gse_t[1:100],scale. = TRUE)
std_dev <- prComp$sdev
pr_var <- std_dev^2
prop_varex <- pr_var/sum(pr_var)
sum(prop_varex[1:50])

```

```{r}
gse_pca <- prcomp(gse_t)
df_toplot <- data.frame(rejection_status_GSE107509, 
                        pc1 = gse_pca$x[,1], pc2 = gse_pca$x[,2]  )


g <- ggplot(df_toplot, aes(x = pc1, y = pc2, color = rejection_status_GSE107509)) + 
  geom_point() + 
  theme_minimal() 
g

```

PCA
```{r}

#compute variance
gse_pca_var <- gse_pca$sdev^2
pca_cumulative_sum_top_30 = cumsum(gse_pca_var[1:30]/sum(gse_pca_var))

# Convert to Data Frame
pca_top_30_df = data.frame(Principal_Component = seq(1,30), Variability = pca_cumulative_sum_top_30)

# Reducing from 50000 genes to 30 principal components results in around 80% variability.


ggplot(data = pca_top_30_df, aes(x = Principal_Component, y = Variability)) + geom_line() + geom_point() + theme_minimal()

```

```{r}

GSE107509_pca <- prcomp(t(GSE107509))
GSE107509_pca_scaled <- prcomp(t(GSE107509), scale. = TRUE)

```

MODEL
```{r}

# Subset into number of PCA
X_pca_10 = data.frame(GSE107509_pca_scaled$x)[,1:10]
X_pca_25 = data.frame(GSE107509_pca_scaled$x)[,1:25]
X_pca_50 = data.frame(GSE107509_pca_scaled$x)[,1:50]
X_pca_75 = data.frame(GSE107509_pca_scaled$x)[,1:75]
X_pca_100 = data.frame(GSE107509_pca_scaled$x)[,1:100]

y = GSE107509_rejection_status

# Keep track of random forest accuracy for each repeat
rf_pca_10 = c()
rf_pca_25 = c()
rf_pca_50 = c()
rf_pca_75 = c()
rf_pca_100 = c()

# Keep track of accuracy for each fold
rf_pca_10_acc = c()
rf_pca_25_acc = c()
rf_pca_50_acc = c()
rf_pca_75_acc = c()
rf_pca_100_acc = c()


cvK = 5  # number of CV folds
n_sim = 25 ## number of repeats

for (i in 1:n_sim) {
  print(i)

  cvSets = cvTools::cvFolds(nrow(X_pca_10), cvK)  # permute all the data, into 5 folds
  
  # Reset accuracy for each fold
  rf_pca_10_acc = c()
  rf_pca_25_acc = c()
  rf_pca_50_acc = c()
  rf_pca_75_acc = c()
  rf_pca_100_acc = c()
  
  for (j in 1:cvK) {
    # Split data into train and test
    test_id = cvSets$subsets[cvSets$which == j]
    X_test_10 = X_pca_10[test_id, ]
    X_train_10 = X_pca_10[-test_id, ]
    
    X_test_25 = X_pca_25[test_id, ]
    X_train_25 = X_pca_25[-test_id, ]
    
    X_test_50 = X_pca_50[test_id, ]
    X_train_50 = X_pca_50[-test_id, ]
    
    X_test_75 = X_pca_75[test_id, ]
    X_train_75 = X_pca_75[-test_id, ]
    
    X_test_100 = X_pca_100[test_id, ]
    X_train_100 = X_pca_100[-test_id, ]
    
    
    y_test = y[test_id,]
    y_train = y[-test_id,]

    ## Perform Random Forest
    rf_res_10 <- randomForest::randomForest(x = X_train_10, y = as.factor(y_train))
    fit_10 <- predict(rf_res_10, X_test_10)
    
    rf_res_25 <- randomForest::randomForest(x = X_train_25, y = as.factor(y_train))
    fit_25 <- predict(rf_res_25, X_test_25)
    
    rf_res_50 <- randomForest::randomForest(x = X_train_50, y = as.factor(y_train))
    fit_50 <- predict(rf_res_50, X_test_50)
    
    rf_res_75 <- randomForest::randomForest(x = X_train_75, y = as.factor(y_train))
    fit_75 <- predict(rf_res_75, X_test_75)
    
    rf_res_100 <- randomForest::randomForest(x = X_train_100, y = as.factor(y_train))
    fit_100 <- predict(rf_res_100, X_test_100)
    
    # Get accuracy and store
    rf_pca_10_acc[j] = table(fit_10, y_test) %>% diag %>% sum %>% `/`(length(y_test))
    rf_pca_25_acc[j] = table(fit_25, y_test) %>% diag %>% sum %>% `/`(length(y_test))
    rf_pca_50_acc[j] = table(fit_50, y_test) %>% diag %>% sum %>% `/`(length(y_test))
    rf_pca_75_acc[j] = table(fit_75, y_test) %>% diag %>% sum %>% `/`(length(y_test))
    rf_pca_100_acc[j] = table(fit_100, y_test) %>% diag %>% sum %>% `/`(length(y_test))
    }
  
  # Store mean accuracy for each repeat
  rf_pca_10 <- append(rf_pca_10, mean(rf_pca_10_acc))
  rf_pca_25 <- append(rf_pca_25, mean(rf_pca_25_acc))
  rf_pca_50 <- append(rf_pca_50, mean(rf_pca_50_acc))
  rf_pca_75 <- append(rf_pca_75, mean(rf_pca_75_acc))
  rf_pca_100 <- append(rf_pca_100, mean(rf_pca_100_acc))
} ## end for

# Display boxplot
boxplot(list("10" = rf_pca_10, "25" = rf_pca_25, "50" = rf_pca_50, "75" = rf_pca_75, "100" = rf_pca_100))

data = rbind(data.frame(PCA = rep("10", n_sim), Accuracy = rf_pca_10), 
             data.frame(PCA = rep("25", n_sim), Accuracy = rf_pca_25), 
             data.frame(PCA = rep("50", n_sim), Accuracy = rf_pca_50), 
             data.frame(PCA = rep("75", n_sim), Accuracy = rf_pca_75), 
             data.frame(PCA = rep("100", n_sim), Accuracy = rf_pca_100))

ggplot(data = data, aes(x = PCA, y = Accuracy)) + geom_boxplot() + theme_minimal() + labs (x = "PCA", y = "Accuracy") 

# Save results of "data" so we can show it in the app
```

```{r}

rf_res

```


```{r}

train.data<-as.matrix(class = GSE107509_rejection_status, pca$x)
train.data <- train.data[,1:30]
metric <- "Accuracy"
control <- trainControl(method="repeatedcv", number=5, repeats=25)
mtry <- sqrt(ncol(train.data))
tunegrid <- expand.grid(.mtry=mtry)
library(doMC)

registerDoMC(cores = 4)
model_rf <- train(class ~ .,data=train.data, method="rf", metric=metric, tuneGrid=tunegrid, trControl=control)

```


```{r}

X_iris_2 = as.matrix(prcomp(iris[-5])$x[,1:2])
X_iris_3 = as.matrix(prcomp(iris[-5])$x[,1:3])
X_iris_4 = as.matrix(prcomp(iris[-5])$x[,1:4])
X_iris_normal = iris[-5]
y_iris = iris[5]

cvK = 5  # number of CV folds
iris_cv2 = c()
iris_cv3 = c()
iris_cv4 = c()
iris_cv_normal = c()

cv_acc_2 = c()
cv_acc_3 = c()
cv_acc_4 = c()
cv_acc_normal = c()

n_sim = 50 ## number of repeats
for (i in 1:n_sim) {

  cvSets = cvTools::cvFolds(nrow(X_iris_2), cvK)  # permute all the data, into 5 folds
  cv_acc_2 = c()
  cv_acc_3 = c()
  cv_acc_4 = c()
  iris_acc_normal = c()
  
  for (j in 1:cvK) {
    test_id = cvSets$subsets[cvSets$which == j]
    
    X_test_2 = X_iris_2[test_id, ]
    X_train_2 = X_iris_2[-test_id, ]
    
    X_test_3 = X_iris_3[test_id, ]
    X_train_3 = X_iris_3[-test_id, ]
    
    X_test_4 = X_iris_4[test_id, ]
    X_train_4 = X_iris_4[-test_id, ]
    
    X_test_normal = X_iris_normal[test_id, ]
    X_train_normal = X_iris_normal[-test_id, ]
    
    y_test = y_iris[test_id,]
    y_train = y_iris[-test_id,]

    ## RandomForest
    rf_res_2 <- randomForest::randomForest(x = X_train_2, y = as.factor(y_train))
    fit2 <- predict(rf_res_2, X_test_2)
    
    rf_res_3 <- randomForest::randomForest(x = X_train_3, y = as.factor(y_train))
    fit3 <- predict(rf_res_3, X_test_3)
    
    rf_res_4 <- randomForest::randomForest(x = X_train_4, y = as.factor(y_train))
    fit4 <- predict(rf_res_4, X_test_4)
    
    rf_res_normal <- randomForest::randomForest(x = X_train_normal, y = as.factor(y_train))
    fit_normal <- predict(rf_res_normal, X_test_normal)
    
    cv_acc_2[j] = table(fit2, y_test) %>% diag %>% sum %>% `/`(length(y_test))
    cv_acc_3[j] = table(fit3, y_test) %>% diag %>% sum %>% `/`(length(y_test))
    cv_acc_4[j] = table(fit4, y_test) %>% diag %>% sum %>% `/`(length(y_test))
    cv_acc_normal[j] = table(fit_normal, y_test) %>% diag %>% sum %>% `/`(length(y_test))
  }
  iris_cv2 <- append(iris_cv2, mean(cv_acc_2))
  iris_cv3 <- append(iris_cv3, mean(cv_acc_3))
  iris_cv4 <- append(iris_cv4, mean(cv_acc_4))
  iris_cv_normal <- append(iris_cv_normal, mean(cv_acc_normal))
} ## end for

boxplot(list(iris_2 = iris_cv2, iris_3 = iris_cv3, iris_4 = iris_cv4, iris_normal = iris_cv_normal)) 
```


```{r}

largevar = apply(GSE107509, 1, var)
ind = which(largevar > quantile(largevar, 0.9))

X = as.matrix(t(GSE107509[ind,]))
y = GSE107509_rejection_status

cvK = 5  # number of CV folds
cv_50acc5_knn = cv_50acc5_svm = cv_50acc5_rf = c()
cv_acc_knn = cv_acc_svm = cv_acc_rf = c()

n_sim = 25 ## number of repeats
for (i in 1:n_sim) {

  cvSets = cvTools::cvFolds(nrow(X), cvK)  # permute all the data, into 5 folds
  cv_acc_knn = cv_acc_svm = cv_acc_rf = c()
  
  for (j in 1:cvK) {
    test_id = cvSets$subsets[cvSets$which == j]
    X_test = X[test_id, ]
    X_train = X[-test_id, ]
    y_test = y[test_id,]
    y_train = y[-test_id,]
    
    ## KNN
    fit5 = class::knn(train = X_train, test = X_test, cl = y_train, k = 5)
    cv_acc_knn[j] = table(fit5, y_test) %>% diag %>% sum %>% `/`(length(y_test))
    
    ## SVM
    svm_res <- e1071::svm(x = X_train, y = as.factor(y_train))
    fit <- predict(svm_res, X_test)
    cv_acc_svm[j] = table(fit, y_test) %>% diag %>% sum %>% `/`(length(y_test))

    ## RandomForest
    rf_res <- randomForest::randomForest(x = X_train, y = as.factor(y_train))
    fit <- predict(rf_res, X_test)
    cv_acc_rf[j] = table(fit, y_test) %>% diag %>% sum %>% `/`(length(y_test))
  }
  cv_50acc5_knn <- append(cv_50acc5_knn, mean(cv_acc_knn))
  cv_50acc5_svm <- append(cv_50acc5_svm, mean(cv_acc_svm))
  cv_50acc5_rf <- append(cv_50acc5_rf, mean(cv_acc_rf))
} ## end for


boxplot(list(SVM = cv_50acc5_svm, KNN = cv_50acc5_knn , RF= cv_50acc5_rf ))


```
Apply to test
```{r}

test_index = c(seq(650,659))

test_outcome = GSE107509_rejection_status[test_index,]
test_data = GSE107509_t[test_index,]

print(test_outcome)

test.data<-predict(pca, newdata = test_data)
test.data <- as.data.frame(test.data)
test.data <- test.data[,1:100]
pred_test <- predict(rf_res, test.data)
pred_test

confusionMatrix(pred_test, test_outcome)


```


```{r}
# Mean gene expression data
mean_df = as.data.frame(apply(gse, 1, mean, na.rm=TRUE))

var_df = as.data.frame(apply(gse, 1, var, na.rm=TRUE))

# can use min and max 

write.csv(mean_df, "GSE107509_mean.csv")
write.csv(var_df, "GSE107509_var.csv")

```


```{r}

# Get top 5 genes by variance
var_df %>% arrange(desc(Var)) %>% head %>% select(Gene)

```



```{r}


# Single sample data example
this_df = as.data.frame(gse[,1])

combined_df = cbind(mean_df, this_df)

colnames(combined_df) = c("mean", "sample")

combined_df = combined_df %>% mutate(comparison = ifelse(mean > sample, "lower", "greater"))

table(combined_df$comparison)


#for gse 1 - Excellence
#greater   lower 
#    871   53844 


#for gse 659 - Rejection
#greater   lower 
#  54397     318 
  
```


```{r}

# Convert into 6*100 samples and 1*59 samples of 50K+ genes

subset = as.data.frame(gse[,601:659])
rownames(subset) = gene_names

write.csv(subset, 'data/GSE107509_Split/GSE107509_Part7.txt', row.names = FALSE)


```

```{r}

# Concatenating the files

# Directory for GSE107509
directory = "data/GSE107509_Split/"

# Read in the files
fileNames <- list.files(directory)

# Read in all files to make a table
GSE107509 = as.data.frame(readr::read_csv("data/GSE107509_Split/GSE107509_GeneNames.txt"))

# Skip First (GeneName) data
for(i in 2:length(fileNames)){
print(fileNames[i])
  temptable <- readr::read_csv(file.path(directory, fileNames[i]))
  # Concatenate the second column (This particular person)
  GSE107509 <- cbind(GSE107509, temptable)
}

# Get gene names
gene_names = GSE107509$Gene

# Remove first column - The X column we saved
GSE107509 = GSE107509[,-1]

# Set gene names as rows
rownames(GSE107509) = gene_names



```



https://stats.stackexchange.com/questions/258938/pca-before-random-forest-regression-provide-better-predictive-scores-for-my-data/258942

- PCA for better prediciton in Random Forests


I think you just answered yourself. In general RF are not good in high dimensional settings or when you have more features than samples, therefore reducing your features from 400 to 8 will help, especially if you have lot's of noisy collinear features. You have also less chance to overfit in this case, but beware of double-dipping and model selection bias. So that you run lot's of models and choose the best one, which might be best just by chance and wouldn't generalize on unseen data.

https://www.rpubs.com/aprasar/293450

